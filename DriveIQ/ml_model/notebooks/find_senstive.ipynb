{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sensitive locations from Overpass API...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set boundaries for latitude and longitude based on your driving data\n",
    "BOUNDARY_MIN_LAT = 39.96  # Adjust as per your driving_data.csv\n",
    "BOUNDARY_MAX_LAT = 40.06  # Adjust as per your driving_data.csv\n",
    "BOUNDARY_MIN_LON = -83.05  # Adjust as per your driving_data.csv\n",
    "BOUNDARY_MAX_LON = -82.85  # Adjust as per your driving_data.csv\n",
    "\n",
    "# Overpass API endpoint\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# Overpass query to fetch sensitive locations including schools, hospitals, libraries, etc.\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"amenity\"=\"school\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"hospital\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"college\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"university\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"nursing_home\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"library\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"police\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"fire_station\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    "  node[\"amenity\"=\"pharmacy\"]({BOUNDARY_MIN_LAT},{BOUNDARY_MIN_LON},{BOUNDARY_MAX_LAT},{BOUNDARY_MAX_LON});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "def fetch_sensitive_locations():\n",
    "    \"\"\"Fetch sensitive locations from the Overpass API.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(OVERPASS_URL, data={'data': overpass_query})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data from Overpass API: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_locations(osm_data):\n",
    "    \"\"\"Parse the Overpass API JSON data and extract relevant location information.\"\"\"\n",
    "    locations = []\n",
    "    for element in osm_data['elements']:\n",
    "        lat = element['lat']\n",
    "        lon = element['lon']\n",
    "        amenity = element['tags'].get('amenity', 'Unknown')\n",
    "        name = element['tags'].get('name', 'Unnamed')\n",
    "        locations.append({\n",
    "            'Latitude': lat,\n",
    "            'Longitude': lon,\n",
    "            'Amenity': amenity,\n",
    "            'Name': name\n",
    "        })\n",
    "    return locations\n",
    "\n",
    "def save_to_csv(locations, output_csv):\n",
    "    \"\"\"Save the parsed location data to a CSV file.\"\"\"\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['Latitude', 'Longitude', 'Amenity', 'Name'])\n",
    "        writer.writeheader()\n",
    "        for location in locations:\n",
    "            writer.writerow(location)\n",
    "    print(f\"Sensitive locations saved to {output_csv}\")\n",
    "\n",
    "def plot_sensitive_areas_on_map(locations, output_html):\n",
    "    \"\"\"Plot sensitive areas on an interactive map using Folium.\"\"\"\n",
    "    # Center the map at the first sensitive location\n",
    "    if not locations:\n",
    "        print(\"No locations found to plot.\")\n",
    "        return\n",
    "    \n",
    "    first_location = locations[0]\n",
    "    folium_map = folium.Map(location=[first_location['Latitude'], first_location['Longitude']], zoom_start=12)\n",
    "    \n",
    "    # Use MarkerCluster to group markers\n",
    "    marker_cluster = MarkerCluster().add_to(folium_map)\n",
    "    \n",
    "    for location in locations:\n",
    "        popup_text = f\"{location['Amenity'].title()}: {location['Name']}\"\n",
    "        folium.Marker(\n",
    "            location=[location['Latitude'], location['Longitude']],\n",
    "            popup=popup_text,\n",
    "            icon=folium.Icon(color='blue', icon='info-sign')\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Save map to HTML file\n",
    "    folium_map.save(output_html)\n",
    "    print(f\"Map saved as {output_html}\")\n",
    "\n",
    "def plot_histogram(data, column, title, xlabel, ylabel):\n",
    "    \"\"\"Plot histogram of a specific column from the data.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[column], bins=30, kde=True, color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "def process_driving_data(input_csv, output_csv, output_map_html):\n",
    "    \"\"\"Main function to process driving data and fetch sensitive locations.\"\"\"\n",
    "    # Fetch sensitive locations from OSM\n",
    "    print(\"Fetching sensitive locations from Overpass API...\")\n",
    "    osm_data = fetch_sensitive_locations()\n",
    "\n",
    "    if osm_data:\n",
    "        print(\"Parsing location data...\")\n",
    "        locations = parse_locations(osm_data)\n",
    "        \n",
    "        # Save sensitive locations to CSV\n",
    "        print(f\"Saving sensitive locations to {output_csv}...\")\n",
    "        save_to_csv(locations, output_csv)\n",
    "\n",
    "        # Plot sensitive areas on a map with popups\n",
    "        print(\"Plotting sensitive areas on a map...\")\n",
    "        plot_sensitive_areas_on_map(locations, output_map_html)\n",
    "\n",
    "        # Load driving data and plot histograms for speed and acceleration\n",
    "        print(\"Loading driving data...\")\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Plot histograms for speed and acceleration\n",
    "        print(\"Plotting histograms for Speed and Acceleration...\")\n",
    "        plot_histogram(df, 'Speed(km/h)', 'Distribution of Speed (km/h)', 'Speed (km/h)', 'Frequency')\n",
    "        plot_histogram(df, 'Acceleration(m/s^2)', 'Distribution of Acceleration (m/s²)', 'Acceleration (m/s²)', 'Frequency')\n",
    "\n",
    "        print(\"Process completed successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve sensitive locations.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = '../data/driving_data.csv'  # Path to your input driving data CSV file\n",
    "    output_csv = '../data/sensitive_location.csv'  # Output CSV for sensitive areas\n",
    "    output_map_html = 'sensitive_areas_map.html'  # Output HTML for map visualization\n",
    "\n",
    "    # Process driving data and detect sensitive areas\n",
    "    process_driving_data(input_csv, output_csv, output_map_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Jerk(m/s^3)', 'Braking_Intensity'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistance(m)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: haversine(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLat_Shifted\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLon_Shifted\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Filter out extreme values for acceleration (beyond ±10 m/s²) and other columns using z-score\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m z_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(stats\u001b[38;5;241m.\u001b[39mzscore(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpeed(m/s)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAcceleration(m/s^2)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJerk(m/s^3)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBraking_Intensity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     54\u001b[0m df \u001b[38;5;241m=\u001b[39m df[(z_scores \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]  \u001b[38;5;66;03m# Keep data within 3 standard deviations\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Recalculate acceleration after outlier removal\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anush\\OneDrive\\Desktop\\minor project\\DriveIQ\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\anush\\OneDrive\\Desktop\\minor project\\DriveIQ\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anush\\OneDrive\\Desktop\\minor project\\DriveIQ\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Jerk(m/s^3)', 'Braking_Intensity'] not in index\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the 'data' directory exists\n",
    "output_directory = 'data'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Load the driving data\n",
    "df = pd.read_csv('../data/driving_data.csv')\n",
    "\n",
    "# Ensure that Time_Step, Latitude, and Longitude are numeric\n",
    "df['Time_Step'] = pd.to_numeric(df['Time_Step'], errors='coerce')\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in important columns\n",
    "df = df.dropna(subset=['Time_Step', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Convert Speed from km/h to m/s\n",
    "df['Speed(m/s)'] = df['Speed(km/h)'] * 0.27778\n",
    "\n",
    "# Haversine formula to calculate distance between two GPS points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    d_lat = radians(lat2 - lat1)\n",
    "    d_lon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "\n",
    "    a = sin(d_lat / 2)**2 + cos(lat1) * cos(lat2) * sin(d_lon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    return R * c * 1000  # Distance in meters\n",
    "\n",
    "# Calculate differences for time, latitude, and longitude\n",
    "df['Time_Diff'] = df['Time_Step'].diff().fillna(0)\n",
    "\n",
    "# Shift latitude and longitude for distance calculation\n",
    "df['Lat_Shifted'] = df['Latitude'].shift(1)\n",
    "df['Lon_Shifted'] = df['Longitude'].shift(1)\n",
    "\n",
    "# Calculate distance between consecutive points\n",
    "df['Distance(m)'] = df.apply(lambda row: haversine(row['Lat_Shifted'], row['Lon_Shifted'], row['Latitude'], row['Longitude']), axis=1)\n",
    "\n",
    "# Filter out extreme values for acceleration (beyond ±10 m/s²) and other columns using z-score\n",
    "z_scores = np.abs(stats.zscore(df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity']].fillna(0)))\n",
    "df = df[(z_scores < 3).all(axis=1)]  # Keep data within 3 standard deviations\n",
    "\n",
    "# Recalculate acceleration after outlier removal\n",
    "df['Acceleration(m/s^2)'] = df['Speed(m/s)'].diff() / df['Time_Diff'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "# Calculate jerk (m/s³)\n",
    "df['Jerk(m/s^3)'] = df['Acceleration(m/s^2)'].diff() / df['Time_Diff'].replace(0, np.nan).fillna(1)\n",
    "\n",
    "# Calculate braking intensity (absolute value of negative acceleration)\n",
    "df['Braking_Intensity'] = df['Acceleration(m/s^2)'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "\n",
    "# Load sensitive locations (school, hospital, etc.)\n",
    "sensitive_locations = pd.read_csv('../data/sensitive_location.csv')\n",
    "\n",
    "# Function to calculate SASV (Sensitive Area Speed Violation)\n",
    "def haversine_vectorized(lat1, lon1, lat2_series, lon2_series):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    d_lat = np.radians(lat2_series - lat1)\n",
    "    d_lon = np.radians(lon2_series - lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2_series = np.radians(lat2_series)\n",
    "    a = np.sin(d_lat / 2)**2 + np.cos(lat1) * np.cos(lat2_series) * np.sin(d_lon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c * 1000  # Distance in meters\n",
    "\n",
    "def calculate_sasv(lat, lon, speed, sensitive_locations):\n",
    "    sensitive_distances = haversine_vectorized(lat, lon, sensitive_locations['Latitude'], sensitive_locations['Longitude'])\n",
    "    if np.any(sensitive_distances < 300):  # Within 300 meters of sensitive areas\n",
    "        if speed > 8.33:  # Speed > 30 km/h in sensitive area\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Apply SASV calculation\n",
    "df['SASV'] = df.apply(lambda row: calculate_sasv(row['Latitude'], row['Longitude'], row['Speed(m/s)'], sensitive_locations), axis=1)\n",
    "\n",
    "# Calculate rule violation score for exceeding general speed limit\n",
    "def calculate_speed_violation(row):\n",
    "    speed_limit = 13.89  # ~50 km/h general speed limit\n",
    "    if row['Speed(m/s)'] > speed_limit:\n",
    "        return 1  # Speed violation\n",
    "    return 0\n",
    "\n",
    "df['Speed_Violation'] = df.apply(calculate_speed_violation, axis=1)\n",
    "\n",
    "# ---------- Driving Score Calculation ---------- #\n",
    "df['Driving_Score'] = 100\n",
    "\n",
    "# Normalize the key features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity']] = scaler.fit_transform(\n",
    "    df[['Speed(m/s)', 'Acceleration(m/s^2)', 'Jerk(m/s^3)', 'Braking_Intensity']])\n",
    "\n",
    "# Apply penalties based on normalized features\n",
    "df['Driving_Score'] -= df['Speed(m/s)'] * 25  # Speed penalty\n",
    "df['Driving_Score'] -= df['Acceleration(m/s^2)'] * 15  # Acceleration penalty\n",
    "df['Driving_Score'] -= df['Jerk(m/s^3)'] * 5  # Jerk penalty\n",
    "df['Driving_Score'] -= df['Braking_Intensity'] * 5  # Braking intensity penalty\n",
    "\n",
    "# Penalty for violations\n",
    "df['Driving_Score'] -= df['SASV'] * 15  # Penalty for violating sensitive areas\n",
    "df['Driving_Score'] -= df['Speed_Violation'] * 15  # Penalty for general speed violation\n",
    "\n",
    "# Ensure the score is within the range [0, 100]\n",
    "df['Driving_Score'] = df['Driving_Score'].clip(upper=100, lower=0)\n",
    "\n",
    "# Driving Category based on score\n",
    "def categorize_driving_score(score):\n",
    "    if score > 80:\n",
    "        return 'Safe'\n",
    "    elif score > 60:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Risky'\n",
    "\n",
    "df['Driving_Category'] = df['Driving_Score'].apply(categorize_driving_score)\n",
    "\n",
    "# Save the processed data\n",
    "processed_columns = ['TripId', 'Time_Step', 'Latitude', 'Longitude', 'Speed(m/s)', 'Acceleration(m/s^2)', \n",
    "                     'Jerk(m/s^3)', 'Braking_Intensity', 'SASV', 'Speed_Violation', 'Driving_Score', 'Driving_Category']\n",
    "processed_data = df[processed_columns]\n",
    "processed_data.to_csv('../data/processed_data.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved to '../data/processed_data.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
